# -*- coding: utf-8 -*-
"""ZAS dataset project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r4mzJh4WMGY-25xTC_noX_v8E1xysv_b
"""

# Commented out IPython magic to ensure Python compatibility.
# Opening files
import os

# Regular expressions
import re

# Working with dataframes
import pandas as pd
import numpy as np

# Preprocessing
import string

!pip install nltk
from nltk.tokenize import word_tokenize, wordpunct_tokenize
import nltk

from nltk import bigrams, WordNetLemmatizer
from collections import Counter,defaultdict

from nltk import download
download('punkt')


# POS tagging
!pip install spacy

from nltk.collocations import *
# WordNet resource
nltk.download('wordnet')

from nltk.corpus import stopwords

# Downloading stopwords for each language
nltk.download('stopwords')
stop_words = set(stopwords.words('russian','german'))

stop_words_russian = set(stopwords.words('russian'))
stop_words_german = set(stopwords.words('german'))

# Combining two list of words
stop_words_combined = stop_words_russian.union(stop_words_german)

# Adding stopwords
custom_stopwords = set(['0word','0wordv', 'xxx', 'hm','0wordd','und dann','und','der','die','das','m','che', 'xote', 'n', 'kken','fa','ha','ex2','gro','mg','molt','dere', 'e', 'de','krit', 'cher','chut','sit'])
stop_words_combined.update(custom_stopwords)

# Language detector
!pip install lingua-language-detector
from lingua import Language, LanguageDetectorBuilder


# Wordcloud instruments
from wordcloud import WordCloud
import matplotlib.pyplot as plt
# %matplotlib inline

from sklearn.feature_extraction.text import TfidfVectorizer

from google.colab import drive
drive.mount('/content/drive')

data = []

# Reading .cha files
for filename in os.listdir('/content/drive/MyDrive/ZAS Rus'):
    if filename.endswith(".cha"):
        with open(os.path.join('/content/drive/MyDrive/ZAS Rus', filename), 'r', encoding='utf-8') as file:
            content = file.read()

            #  Using regular expressions to search file names
            match = re.search(r'_(\d+)_', filename)
            if match:
                child_id = match.group(1)

                # Appending data to the list
                data.append({'ChildID': child_id, 'Language': 'Russian', 'Content': content})

# Creating dataframe
df1 = pd.DataFrame(data)

data1 = []

# Reading .cha files
for filename in os.listdir('/content/drive/MyDrive/ZAS German'):
    if filename.endswith(".cha"):
        with open(os.path.join('/content/drive/MyDrive/ZAS German', filename), 'r', encoding='utf-8') as file:
            content = file.read()

            # Using regular expressions or other methods to extract relevant information

            match = re.search(r'_(\d+)_', filename)
            if match:
                child_id = match.group(1)

                # Appending data to the list
                data1.append({'ChildID': child_id, 'Language': 'German', 'Content': content})

# Create dataframe
df2 = pd.DataFrame(data1)

# Merge dataframes
merged_df = pd.merge(df1, df2, on='ChildID', how='inner')

# print(merged_df.head())

# Preprocessing of Russian text

def preprocess_text_russian(text):
    if not text or pd.isnull(text):
        return ''

    # Remove lines that start with 'ex' followed by a number
    text = re.sub(r'^\s*\*ex\d+:.*\n', '', text, flags=re.IGNORECASE | re.MULTILINE)

    # Remove text that starts with '@'
    text = re.sub(r'@.*', '', text)

    # Remove deu
    text = re.sub(r'@s:deu', '', text)

    # Remove sentences starting with '%com:
    text = re.sub(r'^\s*%com:[^\n]*\n', '', text, flags=re.IGNORECASE | re.MULTILINE)

    # Remove punctuation except single quotes
    text = ''.join(char if char.isalnum() or char == "'" or char.isspace() else '' for char in text)

    # Remove long sequences of digits like 791207792548
    text = re.sub(r'\b\d{12,}\b', '', text)

    # Remove non-alphanumeric characters except single quotes and hyphens
    text = re.sub(r'[^a-zA-Z0-9\s\'-]', '', text)

    # Remove occurrences of 'chi'
    text = re.sub(r'(?i)chi', '', text)

    # Remove consecutive newlines at the beginning of lines
    text = re.sub(r'^\s+', '', text, flags=re.MULTILINE)

    # Remove stop words
    words = text.split()
    words = [word for word in words if word not in custom_stopwords]

    return ' '.join(words)

#  Applying preprocessing function
merged_df['Content_x'] = merged_df['Content_x'].apply(preprocess_text_russian)

print(merged_df['Content_x'])

print(merged_df['Content_x'][40])

import re
from lingua import Language, LanguageDetectorBuilder

def detect_and_cyrillicize(text):
    tokens = text.split()
    #print("Original Tokens:", tokens)

    languages = [Language.ENGLISH, Language.FRENCH, Language.GERMAN, Language.SPANISH,
                 Language.CROATIAN, Language.SERBIAN, Language.POLISH, Language.BULGARIAN,
                 Language.SLOVAK,Language.SLOVENE]
    detector = LanguageDetectorBuilder.from_languages(*languages).build()

    unified_text = ""

    def custom_cyrillicize(token):
        cyrillic_mapping = {
            'a': 'а', 'b': 'б', 'c': 'ц', 'd': 'д', 'e': 'е', 'f': 'ф', 'g': 'г',
            'h': 'х', 'i': 'и', 'j': 'й', 'k': 'к', 'l': 'л', 'm': 'м', 'n': 'н',
            'o': 'о', 'p': 'п', 'q': 'к', 'r': 'р', 's': 'с', 't': 'т', 'u': 'у',
            'v': 'в', 'w': 'в', 'x': 'х', 'y': 'й', 'z': 'з',
            'A': 'А', 'B': 'Б', 'C': 'Ц', 'D': 'Д', 'E': 'Е', 'F': 'Ф', 'G': 'Г',
            'H': 'Х', 'I': 'И', 'J': 'Й', 'K': 'К', 'L': 'Л', 'M': 'М', 'N': 'Н',
            'O': 'О', 'P': 'П', 'Q': 'К', 'R': 'Р', 'S': 'С', 'T': 'Т', 'U': 'У',
            'V': 'В', 'W': 'В', 'X': 'КС', 'Y': 'Й', 'Z': 'З', 'dy': 'ды', 'chk': 'чк',
            'ry': 'ры', 'ju': 'ю', 'che': 'че', 'zh': 'ж', 'shch': 'щ', '0o': 'о',
            'by': 'бы', 'sh': 'ш', 'ja': 'я', 'ch': 'ч', 'nja': 'ня', 'ish': 'иш',
            'sj': 'съ', 'eto': 'это','eta': 'эта', 'etu': 'эту','je': 'е', 'jo': 'е','ja': 'я',
            'len\'': 'лень','toto': 'то-то', 'sja': 'ся','eshche': 'еще', 'cy': 'цы',
            't\'': 'ть', 'l\'': 'ль', 'l\'o': 'ле','sja': 'ся','0n': 'он','zy': 'зы',
            'jaja': 'яя', 's\'': 'сь', 'r\'': 'рь','l\'a': 'ля', 'sh\'': 'шь',
        }

        # Handling 'chk' and 'dy' conversions using regular expression
        cyrillicized_token = re.sub(r'chk|dy|ry|ju|che|zh|0o|by|sh|ja|ch|nja|ish|sj|eto|eta|etu|je|jo|ja|len\'|toto|sja|eshche|cy|t\'|l\'|l\'o|sja|0n|zy|jaja|s\'|r\'|l\'a|sh\'', lambda match: cyrillic_mapping[match.group()], token)
        cyrillicized_token = ''.join(cyrillic_mapping.get(char, char) for char in cyrillicized_token)
        return cyrillicized_token

    for token in tokens:
        confidence_values = detector.compute_language_confidence_values(token)
        detected_language = max(confidence_values, key=lambda x: x.value).language
        #print(f"Token: {token}, Detected Language: {detected_language}, Confidence: {confidence_values[0].value}")

        if detected_language == Language.GERMAN:
            #print(f"Skipping cyrillicization for German token: {token}")
            unified_text += token + " "
        else:
            # Perform cyrillicization only for non-German tokens
            cyrillicized_token = custom_cyrillicize(token)
            #print(f"Cyrillicized Token: {cyrillicized_token}")
            unified_text += cyrillicized_token + " "

    return unified_text.strip()

# Apply cyrillization using cyrillicize function to each element in 'Content_x'
merged_df['russian_text'] = merged_df['Content_x'].apply(lambda text: detect_and_cyrillicize(text))

print(merged_df['Content_x'][42])
print(merged_df['russian_text'][42])

print(merged_df['Content_x'][36])
print(merged_df['russian_text'][36])

import spacy
from lingua import Language, LanguageDetectorBuilder
!python -m spacy download de_core_news_lg

# Initializing spaCy with a German language model
nlp = spacy.load('de_core_news_lg')

# Function to identify if the word is German
def is_german(word):
    languages = [Language.ENGLISH,Language.CZECH, Language.BOSNIAN, Language.POLISH, Language.GERMAN, Language.CROATIAN,
    Language.ROMANIAN, Language.FRENCH, Language.SERBIAN,Language.SPANISH, Language.BULGARIAN, Language.SLOVAK]
    detector = LanguageDetectorBuilder.from_languages(*languages).build()
    confidence_values = detector.compute_language_confidence_values(word)
    detected_language = max(confidence_values, key=lambda x: x.value).language
    return detected_language == Language.GERMAN

# Function to get part of speech for a word
def get_pos(word):
    doc = nlp(word)
    return doc[0].pos_ if doc else None

# Dictionary to store unique German words and their POS counts
unique_german_words = {}

# Applying the function to the 'russian_text' column
for idx, text in enumerate(merged_df['russian_text']):
    tokens = text.split()

    for token in tokens:
        if is_german(token):
            pos = get_pos(token)
            if pos:
                if token in unique_german_words:
                    unique_german_words[token][pos] = unique_german_words[token].get(pos, 0) + 1
                else:
                    unique_german_words[token] = {pos: 1}

# Displaying unique German words, their POS, and count
print("Unique German Words, POS, and Count:")
for word, pos_counts in unique_german_words.items():
    for pos, count in pos_counts.items():
        print(f"Word: {word}, POS: {pos}, Count: {count}")

import spacy
from lingua import Language, LanguageDetectorBuilder
!pip install hmmlearn
from hmmlearn import hmm
import numpy as np

# getting labels for words and saving them in df

# Function to identify if the word is German
def is_german(word):
    languages = [Language.ENGLISH, Language.GERMAN, Language.FRENCH, Language.RUSSIAN, Language.SPANISH, Language.CROATIAN, Language.SERBIAN, Language.POLISH, Language.BULGARIAN, Language.SLOVAK]
    detector = LanguageDetectorBuilder.from_languages(*languages).build()
    confidence_values = detector.compute_language_confidence_values(word)
    detected_language = max(confidence_values, key=lambda x: x.value).language
    return detected_language == Language.GERMAN

# Function to get part of speech for a word
def get_pos(word):
    doc = nlp(word)
    return doc[0].pos_ if doc else None

# Function to label each word as Russian or German
def label_language(row):
    tokens = row['russian_text'].split()
    languages = []
    words = []

    for token in tokens:
        if is_german(token):
            languages.append('German')
        else:
            languages.append('Russian')
        words.append(token)

    row['language'] = languages
    row['words'] = words
    return row

# Apply the label_language function to each row in the DataFrame
merged_df = merged_df.apply(label_language, axis=1)

# Create a new DataFrame for 'language' and 'words'
language_words_df = pd.DataFrame(merged_df[['language', 'words']])

# Display the new DataFrame
print(language_words_df)

# Save the new DataFrame to a CSV file
language_words_df.to_csv('language_words_df.csv', index=False)

# Initialize spaCy with a German language model
nlp = spacy.load('de_core_news_lg')

# Encode words and labels as integers
unique_words = set(word for word_labels in language_words_df['language'] for word in word_labels)
word_to_int = {word: i for i, word in enumerate(unique_words)}
label_to_int = {'Russian': 0, 'German': 1}

X = [[word_to_int[word] for word in word_labels] for word_labels in language_words_df['language']]
y = [label_to_int[label] for word_labels in language_words_df['language'] for label in word_labels]


# Tokenize the text from merged_df['russian_text'][42]
sample_sentence = merged_df['russian_text'].iloc[42]
tokens = sample_sentence.split()

# Flatten the word_labels and create a corresponding list of language labels
flat_labels = [label_to_int[label] for label in language_words_df['language'].explode()]

# Reshape the data for training
X_flat = np.array(flat_labels).reshape(-1, 1)

# Set the number of hidden states
n_states = 2

# Create an HMM model
model = hmm.MultinomialHMM(n_components=n_states)

# Train the HMM model

# Function to predict the language of the next word based on the previous word's language
def predict_language(model, previous_language, label_to_int, current_word, current_label):
    # Encode the previous language
    model.fit(X_flat)

    prev_language_encoded = label_to_int.get(previous_language, -1)
    if prev_language_encoded == -1 and previous_language is not None:
        print(f"Warning: '{previous_language}' is not in the vocabulary.")
        return None

    # Predict the next state probabilities
    state_probs = model.predict_proba([[prev_language_encoded]])

    # Get the label corresponding to the state with the highest probability
    predicted_label = list(label_to_int.keys())[np.argmax(state_probs[0])]

    # Display the probabilities
    prob_german = state_probs[0][label_to_int['German']]
    prob_russian = state_probs[0][label_to_int['Russian']]
    print(f"Word: {current_word}, Current Label: {current_label}, Predicted Language: {predicted_label}, Probability of being German: {prob_german:.4f}, Probability of being Russian: {prob_russian:.4f}")

    return predicted_label

# Apply the function to each word in the sequence
previous_language = language_words_df['language'].explode().iloc[0]

for current_label, current_word in zip(language_words_df['language'].explode(), tokens):
    predicted_language = predict_language(model, previous_language, label_to_int, current_word, current_label)
    if predicted_language is not None:
        print(f"\nThe predicted language of the current word '{current_word}' with current label '{current_label}' based on the previous word's language '{previous_language}' is: {predicted_language}")
        previous_language = predicted_language
    else:
        print(f"Warning: Unable to determine the predicted language.")

# Access transition matrix
transition_matrix = model.transmat_

# Display transition probabilities
print("Transition Probabilities:")
for i, from_state in enumerate(label_to_int.keys()):
    for j, to_state in enumerate(label_to_int.keys()):
        prob = transition_matrix[label_to_int[from_state]][label_to_int[to_state]]
        print(f"Probability of transitioning from {from_state} to {to_state}: {prob:.4f}")

# Preprocessing of German text

import re
import string


def preprocess_text_german(text):
    if not text or pd.isnull(text):
        return ''

    # Remove lines that start with 'ex' followed by a number
    text = re.sub(r'^\s*\*ex\d+:.*\n', '', text, flags=re.IGNORECASE | re.MULTILINE)

    # Remove text that starts with '@'
    text = re.sub(r'@.*', '', text)

    # Remove quotes, lowercase the text, and remove punctuation
    text = text.lower().translate(str.maketrans("", "", string.punctuation))

    # Remove long sequences of digits like 791207792548
    text = re.sub(r'\b\d{12,}\b', '', text)

    # Remove non-alphanumeric characters
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)

    # Remove occurrences of 'chi'
    text = re.sub(r'(?i)chi', '', text)

    # Remove consecutive newlines at the beginning of lines
    text = re.sub(r'^\s+', '', text, flags=re.MULTILINE)


  # Remove German stop words
    words = text.split()
    words = [word for word in words if word not in custom_stopwords]

    return ' '.join(words)


# Apply the preprocessing function to the "Content_y" column
merged_df['german_text'] = merged_df['Content_y'].apply(preprocess_text_german)


# Display the preprocessed DataFrame
print(merged_df['german_text'])

!python -m spacy download de_core_news_lg
import spacy
from spacy.lang.de import German
import re
from nltk.corpus import stopwords
!pip install razdel
from razdel import sentenize

#collocations
stops = set(stopwords.words('german'))
nlp = spacy.load("de_core_news_lg")


def normalize(text):
    doc = nlp(text)
    list_of_lemmas = ' '.join([tok.lemma_ for tok in doc])
    tokens = re.findall(r'\b\w+\b', list_of_lemmas.lower(), flags=re.UNICODE)
    normalized_text = [word for word in tokens if len(word) > 2 and word not in stops]

    return normalized_text


def preprocess(text):
    sents = sentenize(text)
    return [normalize(sent.text) for sent in sents]

def ngrammer(tokens, stops, n=2):
    ngrams = []
    tokens = [token for token in tokens if token not in stops]
    for i in range(0,len(tokens)-n+1):
        ngrams.append(tuple(tokens[i:i+n]))
    return ngrams

import spacy

# Load spaCy model for German
nlp = spacy.load("de_core_news_lg")

# Tokenization function
def tokenize_text(text):
    doc = nlp(text)
    tokenized_sentence = [token.text for token in doc]
    return tokenized_sentence

german_text_column = merged_df['german_text']
russian_text_column = merged_df['russian_text']

# Tokenization
tokenized_german_text = [tokenize_text(sent) for sent in german_text_column]
tokenized_russian_text = [tokenize_text(sent) for sent in russian_text_column]

def scorer_simple(word_count_a, word_count_b, bigram_count, *args):
    try:
        score = bigram_count/((word_count_a+word_count_b))

    except ZeroDivisionError:
        return 0

    return score

#Сделаем функцию, которая будет делать счетчики для слов и биграммов.
def collect_stats(tokenized_german_text,tokenized_russian_text, stops):
    ## соберем статистики для отдельных слов
    ## и биграммов

    unigrams = Counter()
    bigrams = Counter()

    for sent in tokenized_german_text:
        unigrams.update(sent)
        bigrams.update(ngrammer(sent, stops, 2))

    for sent in tokenized_russian_text:
        unigrams.update(sent)
        bigrams.update(ngrammer(sent,stops,2))

    return unigrams, bigrams

#И функцию, которая пройдет по всем биграммам и вычислит для них нашу метрику.
def score_bigrams(unigrams, bigrams, scorer, threshold=-100000, min_count=1):
    ## посчитаем метрику для каждого нграмма
    bigram2score = Counter()
    len_vocab = len(unigrams)
    for bigram in bigrams:
        score = scorer(unigrams[bigram[0]], unigrams[bigram[1]],
                       bigrams[bigram], len_vocab, min_count)

        ## если метрика выше порога, добавляем в словарик
        if score > threshold:
            bigram2score[bigram] = score

    return bigram2score

unigrams, bigrams = collect_stats(tokenized_german_text, tokenized_russian_text, stops)

bigram2score = score_bigrams(unigrams, bigrams, scorer_simple)
bigram2score

from collections import defaultdict
def get_window_stats(texts, window=8):

    bigrams = defaultdict(list)

    # проходим окном по текстам
    # берем первое слово и считаем его целевым
    # проходим по остальным словам и их индексам
    # добавляем в словарь пары (целевое слов, текущее слово)
    # и добавляем индекс текущего в список этой пары
    # так мы получаем (слово_1,слово_2):[1,2,1,1,3,2]
    # порядок в этом случае учитывается - (слово_2, слово_1) - другая запись
    for text in texts:
        for i in range(len(text)-window):
            words = list(enumerate(text[i:i+window]))
            target = words[0][1]
            for j, word in words[1:]:
                bigrams[(target, word)].append(j)

    bigrams_stds = Counter()
    for bigram in bigrams:
        # выкидываем биграмы встретившиеся < 2 раз
        if len(bigrams[bigram]) > 2:
            bigrams_stds[bigram] = np.std(bigrams[bigram])

    return bigrams_stds

#get_window_stats(tokenized_german_text)

bigram_measures = nltk.collocations.BigramAssocMeasures()

finder2 = BigramCollocationFinder.from_documents(tokenized_german_text)

finder2.nbest(bigram_measures.likelihood_ratio, 25)

bigram_measures = nltk.collocations.BigramAssocMeasures()

finder2 = BigramCollocationFinder.from_documents(tokenized_russian_text)

finder2.nbest(bigram_measures.likelihood_ratio, 25)

#scores = finder2.score_ngrams(bigram_measures.raw_freq)
#scores

# Define a lemmatizer
lemmatizer = WordNetLemmatizer()

# Apply lemmatization to 'content_x' and 'content_y' columns
df1['lemmatized_x'] = merged_df['russian_text'].apply(lambda text: [lemmatizer.lemmatize(word) for word in text.split()])
df2['lemmatized_y'] = merged_df['german_text'].apply(lambda text: [lemmatizer.lemmatize(word) for word in text.split()])

# Combine the lemmatized words into two separate lists
lemmas_x = [word for sublist in df1['lemmatized_x'].tolist() if isinstance(sublist, list) for word in sublist]
lemmas_y = [word for sublist in df2['lemmatized_y'].tolist() if isinstance(sublist, list) for word in sublist]

# Print the lists of lemmas
print("Lemmas for russian_text:", lemmas_x)
print("Lemmas for german_text:", lemmas_y)

# Генерируем облако слов
wordcloud = WordCloud().generate(', '.join(merged_df['german_text']))
plt.imshow(wordcloud) # Что изображаем
plt.axis("off") # Без подписей на осях
plt.show() # показать изображение

# Генерируем облако слов
wordcloud = WordCloud().generate(', '.join(merged_df['russian_text']))
plt.imshow(wordcloud) # Что изображаем
plt.axis("off") # Без подписей на осях
plt.show() # показать изображение

import spacy
!python -m spacy download de_core_news_lg

# Load spaCy language model
nlp = spacy.load("de_core_news_lg")

# Your content
german_text = merged_df['german_text'].to_string(index=False)

# Process the content with spaCy
tokens_de = nlp(german_text)

# Extract verbs and lemmatize them
lemmatized_verbs = [token.lemma_ for token in tokens_de if token.pos_ == "VERB"]

# Print the lemmatized verbs
print("Lemmatized Verbs:", lemmatized_verbs)

# Extract tokens and parts of speech
tokens_pos = [(token.text, token.pos_) for token in tokens_de]

# Print tokens and parts of speech
#for token, pos in tokens_pos:
    #print(f"Token: {token}, POS: {pos}")

# Count the occurrences of each part of speech
pos_counter = Counter(pos for _, pos in tokens_pos)

# Print the count of tokens for each part of speech
print("Count of tokens for each part of speech:")
for pos, count in pos_counter.items():
    print(f"POS: {pos}, Count: {count}")

# Get the 20 most common lemmatized verbs
most_common_lemmatized_verbs = Counter(lemmatized_verbs).most_common(20)
print(most_common_lemmatized_verbs)

import matplotlib.pyplot as plt
from collections import Counter

# Assuming you have a list of lemmatized verbs named lemmatized_verbs
most_common_lemmatized_verbs = Counter(lemmatized_verbs).most_common(15)

# Separate the verbs and their counts
verbs, counts = zip(*most_common_lemmatized_verbs)

# Plotting
plt.figure(figsize=(12, 6))
plt.bar(verbs, counts, color='blue')
plt.xlabel('Lemmatized Verbs')
plt.ylabel('Frequency')
plt.title('Top 15 Most Common Lemmatized Verbs')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Find all occurrences of "um" in tokens
#um_occurrences = [token.text for token in tokens_de if "um" in token.text]

# Print the results
#print("Occurrences of 'um' in tokens:", um_occurrences)

from sklearn.feature_extraction.text import TfidfVectorizer

# Combine the lemmas into a single list for TF-IDF analysis
all_lemmas = lemmas_x + lemmas_y

# Convert the lemmas into a string for TF-IDF vectorization
all_text = ' '.join(all_lemmas)

# Create the TF-IDF vectorizer
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform([all_text])

# Get feature names (lemmas) and corresponding TF-IDF values
feature_names = vectorizer.get_feature_names_out()
tfidf_values = tfidf_matrix.toarray().flatten()

# Find the index of the largest TF-IDF value
max_index = tfidf_values.argmax()

# Get the lemma with the largest TF-IDF value
max_lemma = feature_names[max_index]

# Get the corresponding TF-IDF value
max_tfidf_value = tfidf_values[max_index]

# Print the result
print(f"The lemma with the largest TF-IDF value is '{max_lemma}' with TF-IDF value {max_tfidf_value:.4f}")

# Get indices of the top 10 TF-IDF values
top_indices = tfidf_values.argsort()[-10:][::-1]

# Print the 10 largest TF-IDF values and their corresponding lemmas
print("Top 10 TF-IDF values:")
for idx in top_indices:
    lemma = feature_names[idx]
    tfidf_value = tfidf_values[idx]
    print(f"Lemma: '{lemma}', TF-IDF: {tfidf_value:.4f}")

# Count the occurrences of each part of speech
pos_counter = Counter(pos for _, pos in tokens_pos)

# Print the count of tokens for each part of speech
print("Count of tokens for each part of speech:")
for pos, count in pos_counter.items():
    print(f"POS: {pos}, Count: {count}")

import matplotlib.pyplot as plt

# Get indices of the top 10 TF-IDF values
top_indices = tfidf_values.argsort()[-10:][::-1]

# Prepare data for visualization
top_lemmas = [feature_names[idx] for idx in top_indices]
top_tfidf_values = [tfidf_values[idx] for idx in top_indices]

# Plotting the bar chart
plt.figure(figsize=(10, 6))
plt.barh(top_lemmas, top_tfidf_values, color='skyblue')
plt.xlabel('TF-IDF Value')
plt.title('Top 10 TF-IDF Values and Their Corresponding Lemmas')
plt.gca().invert_yaxis()  # Invert y-axis to have the highest TF-IDF at the top
plt.show()

!python -m spacy download ru_core_news_lg


import spacy

# Load spaCy language model for Russian
nlp = spacy.load("ru_core_news_lg")

# Your content
text = merged_df['russian_text'].to_string(index=False)

# Process the content with spaCy
tokens_ru = nlp(text)

# Extract verbs and lemmatize them
lemmatized_verbs = [token.lemma_ for token in tokens_ru if token.pos_ == "VERB"]

# Print the lemmatized verbs
print("Lemmatized Verbs:", lemmatized_verbs)

# Get the 20 most common lemmatized verbs
most_common_lemmatized_verbs = Counter(lemmatized_verbs).most_common(15)
print(most_common_lemmatized_verbs)

import matplotlib.pyplot as plt
from collections import Counter

# Assuming you have a list of lemmatized verbs named lemmatized_verbs
most_common_lemmatized_verbs = Counter(lemmatized_verbs).most_common(15)

# Separate the verbs and their counts
verbs, counts = zip(*most_common_lemmatized_verbs)

# Plotting
plt.figure(figsize=(12, 6))
plt.bar(verbs, counts, color='blue')
plt.xlabel('Lemmatized Verbs')
plt.ylabel('Frequency')
plt.title('Top 15 Most Common Lemmatized Verbs')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Extract tokens and parts of speech
tokens_pos = [(token.text, token.pos_) for token in tokens_ru]

# Print the count of tokens for each part of speech
print("Count of tokens for each part of speech:")
for pos, count in pos_counter.items():
    print(f"POS: {pos}, Count: {count}")

# Count the occurrences of each part of speech
pos_counter = Counter(pos for _, pos in tokens_pos)

# Print the count of tokens for each part of speech
print("Count of tokens for each part of speech:")
for pos, count in pos_counter.items():
    print(f"POS: {pos}, Count: {count}")

#bigrams in german

from nltk import bigrams
german_text = merged_df['german_text'].to_string(index=False)

# Tokenize the text
tokens = word_tokenize(german_text)

# Find bigrams
german_bigrams = list(bigrams(tokens))

# Print the bigrams
print("Bigrams:", german_bigrams)

#bigrams in russian

from nltk import bigrams
russian_text = merged_df['Content_x'].to_string(index=False)

# Tokenize the text
tokens = word_tokenize(russian_text)

# Find bigrams
russian_bigrams = list(bigrams(tokens))

# Print the bigrams
print("Bigrams:", russian_bigrams)

