# -*- coding: utf-8 -*-
"""Preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iiaHcj9BGVzTgJdPFO-cjDVnDlc-xhgu
"""

# Commented out IPython magic to ensure Python compatibility.
# Opening files
import os

# Regular expressions
import re

# Working with dataframes
import pandas as pd
import numpy as np

# Preprocessing
import string

!pip install nltk
from nltk.tokenize import word_tokenize, wordpunct_tokenize
import nltk

from nltk import bigrams, WordNetLemmatizer
from collections import Counter,defaultdict

from nltk import download
download('punkt')


# POS tagging
!pip install spacy

from nltk.collocations import *
# WordNet resource
nltk.download('wordnet')

from nltk.corpus import stopwords

# Downloading stopwords for each language
nltk.download('stopwords')
stop_words = set(stopwords.words('russian','german'))

stop_words_russian = set(stopwords.words('russian'))
stop_words_german = set(stopwords.words('german'))

# Combining two list of words
stop_words_combined = stop_words_russian.union(stop_words_german)

# Adding stopwords
custom_stopwords = set(['0word','0wordv', 'xxx', 'hm','0wordd','und dann','und','der','die','das','m','che', 'xote', 'n', 'kken','fa','ha','ex2','gro','mg','molt','dere', 'e', 'de','krit', 'cher','chut','sit'])
stop_words_combined.update(custom_stopwords)

# Language detector
!pip install lingua-language-detector
from lingua import Language, LanguageDetectorBuilder


# Wordcloud instruments
from wordcloud import WordCloud
import matplotlib.pyplot as plt
# %matplotlib inline

from sklearn.feature_extraction.text import TfidfVectorizer

from google.colab import drive
drive.mount('/content/drive')

data = []

# Reading .cha files
for filename in os.listdir('/content/drive/MyDrive/ZAS Rus'):
    if filename.endswith(".cha"):
        with open(os.path.join('/content/drive/MyDrive/ZAS Rus', filename), 'r', encoding='utf-8') as file:
            content = file.read()

            #  Using regular expressions to search file names
            match = re.search(r'_(\d+)_', filename)
            if match:
                child_id = match.group(1)

                # Appending data to the list
                data.append({'ChildID': child_id, 'Language': 'Russian', 'Content': content})

# Creating dataframe
df1 = pd.DataFrame(data)

data1 = []

# Reading .cha files
for filename in os.listdir('/content/drive/MyDrive/ZAS German'):
    if filename.endswith(".cha"):
        with open(os.path.join('/content/drive/MyDrive/ZAS German', filename), 'r', encoding='utf-8') as file:
            content = file.read()

            # Using regular expressions or other methods to extract relevant information

            match = re.search(r'_(\d+)_', filename)
            if match:
                child_id = match.group(1)

                # Appending data to the list
                data1.append({'ChildID': child_id, 'Language': 'German', 'Content': content})

# Create dataframe
df2 = pd.DataFrame(data1)

# Merge dataframes
merged_df = pd.merge(df1, df2, on='ChildID', how='inner')

# print(merged_df.head())

# Preprocessing of Russian text

def preprocess_text_russian(text):
    if not text or pd.isnull(text):
        return ''

    # Remove lines that start with 'ex' followed by a number
    text = re.sub(r'^\s*\*ex\d+:.*\n', '', text, flags=re.IGNORECASE | re.MULTILINE)

    # Remove text that starts with '@'
    text = re.sub(r'@.*', '', text)

    # Remove deu
    text = re.sub(r'@s:deu', '', text)

    # Remove sentences starting with '%com:
    text = re.sub(r'^\s*%com:[^\n]*\n', '', text, flags=re.IGNORECASE | re.MULTILINE)

    # Remove punctuation except single quotes
    text = ''.join(char if char.isalnum() or char == "'" or char.isspace() else '' for char in text)

    # Remove long sequences of digits like 791207792548
    text = re.sub(r'\b\d{12,}\b', '', text)

    # Remove non-alphanumeric characters except single quotes and hyphens
    text = re.sub(r'[^a-zA-Z0-9\s\'-]', '', text)

    # Remove occurrences of 'chi'
    text = re.sub(r'(?i)chi', '', text)

    # Remove consecutive newlines at the beginning of lines
    text = re.sub(r'^\s+', '', text, flags=re.MULTILINE)

    # Remove stop words
    words = text.split()
    words = [word for word in words if word not in custom_stopwords]

    return ' '.join(words)

#  Applying preprocessing function
merged_df['Content_x'] = merged_df['Content_x'].apply(preprocess_text_russian)

import re
from lingua import Language, LanguageDetectorBuilder

def detect_and_cyrillicize(text):
    tokens = text.split()
    #print("Original Tokens:", tokens)

    languages = [Language.ENGLISH, Language.FRENCH, Language.GERMAN, Language.SPANISH,
                 Language.CROATIAN, Language.SERBIAN, Language.POLISH, Language.BULGARIAN,
                 Language.SLOVAK,Language.SLOVENE]
    detector = LanguageDetectorBuilder.from_languages(*languages).build()

    unified_text = ""

    def custom_cyrillicize(token):
        cyrillic_mapping = {
            'a': 'а', 'b': 'б', 'c': 'ц', 'd': 'д', 'e': 'е', 'f': 'ф', 'g': 'г',
            'h': 'х', 'i': 'и', 'j': 'й', 'k': 'к', 'l': 'л', 'm': 'м', 'n': 'н',
            'o': 'о', 'p': 'п', 'q': 'к', 'r': 'р', 's': 'с', 't': 'т', 'u': 'у',
            'v': 'в', 'w': 'в', 'x': 'х', 'y': 'й', 'z': 'з',
            'A': 'А', 'B': 'Б', 'C': 'Ц', 'D': 'Д', 'E': 'Е', 'F': 'Ф', 'G': 'Г',
            'H': 'Х', 'I': 'И', 'J': 'Й', 'K': 'К', 'L': 'Л', 'M': 'М', 'N': 'Н',
            'O': 'О', 'P': 'П', 'Q': 'К', 'R': 'Р', 'S': 'С', 'T': 'Т', 'U': 'У',
            'V': 'В', 'W': 'В', 'X': 'КС', 'Y': 'Й', 'Z': 'З', 'dy': 'ды', 'chk': 'чк',
            'ry': 'ры', 'ju': 'ю', 'che': 'че', 'zh': 'ж', 'shch': 'щ', '0o': 'о',
            'by': 'бы', 'sh': 'ш', 'ja': 'я', 'ch': 'ч', 'nja': 'ня', 'ish': 'иш',
            'sj': 'съ', 'eto': 'это','eta': 'эта', 'etu': 'эту','je': 'е', 'jo': 'е','ja': 'я',
            'len\'': 'лень','toto': 'то-то', 'sja': 'ся','eshche': 'еще', 'cy': 'цы',
            't\'': 'ть', 'l\'': 'ль', 'l\'o': 'ле','sja': 'ся','0n': 'он','zy': 'зы',
            'jaja': 'яя', 's\'': 'сь', 'r\'': 'рь','l\'a': 'ля', 'sh\'': 'шь',
        }

        # Handling 'chk' and 'dy' conversions using regular expression
        cyrillicized_token = re.sub(r'chk|dy|ry|ju|che|zh|0o|by|sh|ja|ch|nja|ish|sj|eto|eta|etu|je|jo|ja|len\'|toto|sja|eshche|cy|t\'|l\'|l\'o|sja|0n|zy|jaja|s\'|r\'|l\'a|sh\'', lambda match: cyrillic_mapping[match.group()], token)
        cyrillicized_token = ''.join(cyrillic_mapping.get(char, char) for char in cyrillicized_token)
        return cyrillicized_token

    for token in tokens:
        confidence_values = detector.compute_language_confidence_values(token)
        detected_language = max(confidence_values, key=lambda x: x.value).language
        #print(f"Token: {token}, Detected Language: {detected_language}, Confidence: {confidence_values[0].value}")

        if detected_language == Language.GERMAN:
            #print(f"Skipping cyrillicization for German token: {token}")
            unified_text += token + " "
        else:
            # Perform cyrillicization only for non-German tokens
            cyrillicized_token = custom_cyrillicize(token)
            #print(f"Cyrillicized Token: {cyrillicized_token}")
            unified_text += cyrillicized_token + " "

    return unified_text.strip()

# Apply cyrillization using cyrillicize function to each element in 'Content_x'
merged_df['russian_text'] = merged_df['Content_x'].apply(lambda text: detect_and_cyrillicize(text))

# Preprocessing of German text

import re
import string


def preprocess_text_german(text):
    if not text or pd.isnull(text):
        return ''

    # Remove lines that start with 'ex' followed by a number
    text = re.sub(r'^\s*\*ex\d+:.*\n', '', text, flags=re.IGNORECASE | re.MULTILINE)

    # Remove text that starts with '@'
    text = re.sub(r'@.*', '', text)

    # Remove quotes, lowercase the text, and remove punctuation
    text = text.lower().translate(str.maketrans("", "", string.punctuation))

    # Remove long sequences of digits like 791207792548
    text = re.sub(r'\b\d{12,}\b', '', text)

    # Remove non-alphanumeric characters
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)

    # Remove occurrences of 'chi'
    text = re.sub(r'(?i)chi', '', text)

    # Remove consecutive newlines at the beginning of lines
    text = re.sub(r'^\s+', '', text, flags=re.MULTILINE)


  # Remove German stop words
    words = text.split()
    words = [word for word in words if word not in custom_stopwords]

    return ' '.join(words)


# Apply the preprocessing function to the "Content_y" column
merged_df['german_text'] = merged_df['Content_y'].apply(preprocess_text_german)


# Display the preprocessed DataFrame
print(merged_df['german_text'])